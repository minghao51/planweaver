# Plan Comparison Feature Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Enable users to compare multiple strawman proposals side-by-side with execution steps, cost/time estimates, and risk assessments before selecting an approach.

**Architecture:** Hybrid on-demand generation - lightweight analysis included in proposal generation (step count, complexity, time, cost, risks), detailed execution graphs generated only when user requests comparison. Comparison service computes diffs between proposals using fuzzy matching, frontend displays in hybrid mode (diff view by default, expandable to side-by-side).

**Tech Stack:** FastAPI, Pydantic v2, SQLAlchemy, LiteLLM, React, TypeScript, pytest, asyncio

---

## Task 1: Add Lightweight Analysis Models

**Files:**
- Modify: `src/planweaver/models/plan.py`

**Step 1: Read the current models file**

Run: `cat src/planweaver/models/plan.py`

**Step 2: Add ProposalWithAnalysis model**

Add after the existing `Proposal` class definition (around line 100):

```python
class ProposalWithAnalysis(BaseModel):
    """Proposal with lightweight analysis for quick comparison"""

    # Existing fields from Proposal
    proposal_id: int
    title: str
    description: str
    approach: str
    pros: List[str]
    cons: List[str]

    # New lightweight analysis fields
    estimated_step_count: int
    complexity_score: Literal["Low", "Medium", "High"]
    estimated_time_minutes: int
    estimated_cost_usd: Decimal
    risk_factors: List[str]

    class Config:
        from_attributes = True
```

**Step 3: Add comparison models at end of file**

Add before the final `# End of models` comment:

```python
class StepSummary(BaseModel):
    """Simplified step for comparison display"""
    task: str
    complexity: Literal["Low", "Medium", "High"]
    estimated_time_minutes: int


class ProposalDetail(BaseModel):
    """Full proposal with execution graph for comparison"""
    proposal_id: int
    full_execution_graph: List[ExecutionStep]
    accurate_time_estimate: int
    accurate_cost_estimate: Decimal
    all_risk_factors: List[str]
    generation_error: Optional[str] = None


class ProposalComparison(BaseModel):
    """Detailed comparison of selected proposals"""
    session_id: str
    proposals: List[ProposalDetail]
    common_steps: List[StepSummary]
    unique_steps_by_proposal: Dict[int, List[StepSummary]]
    time_comparison: Dict[int, int]
    cost_comparison: Dict[int, Decimal]
    complexity_comparison: Dict[int, str]


class ComparisonRequest(BaseModel):
    """Request to compare proposals"""
    proposal_ids: List[int]
```

**Step 4: Verify models compile**

Run: `uv run python -c "from planweaver.models.plan import ProposalWithAnalysis, ProposalComparison; print('Models imported successfully')"`

Expected: No errors, "Models imported successfully" printed

**Step 5: Commit**

```bash
git add src/planweaver/models/plan.py
git commit -m "feat: add comparison models (ProposalWithAnalysis, ProposalComparison)"
```

---

## Task 2: Enhance Planner with Lightweight Analysis

**Files:**
- Modify: `src/planweaver/services/planner.py`
- Test: `tests/test_planner.py`

**Step 1: Read current planner implementation**

Run: `cat src/planweaver/services/planner.py`

**Step 2: Add lightweight analysis method**

Add before the `_generate_planner_prompt` method (around line 150):

```python
    async def _analyze_proposals_lightweight(
        self,
        plan: Plan,
        proposals: List[dict]
    ) -> Dict[int, dict]:
        """Generate lightweight analysis for proposals without full execution graph.

        Uses fast LLM to estimate:
        - Step count
        - Complexity score
        - Estimated time (based on ~2min per step)
        - Estimated cost (based on ~$0.001 per step)
        - Risk factors

        Returns: Dict mapping proposal_id to analysis data
        """
        prompt = f"""You are analyzing planning proposals for complexity and risk.

User Intent: {plan.user_intent}

Proposals to analyze:
{self._format_proposals_for_analysis(proposals)}

For EACH proposal, provide:
1. estimated_step_count: Number of execution steps (integer, typically 3-15)
2. complexity_score: "Low", "Medium", or "High" based on technical complexity
3. estimated_time_minutes: Total time in minutes (assume ~2 minutes per step average)
4. estimated_cost_usd: Cost in USD (assume ~$0.001 per step average)
5. risk_factors: List of 2-3 specific risks or challenges

Return JSON only, no explanation:
{{
  "1": {{"estimated_step_count": 5, "complexity_score": "Medium", "estimated_time_minutes": 10, "estimated_cost_usd": 0.005, "risk_factors": ["API rate limits", "Data migration"]}},
  "2": {{"estimated_step_count": 3, "complexity_score": "Low", "estimated_time_minutes": 6, "estimated_cost_usd": 0.003, "risk_factors": ["Configuration errors"]}}
}}
"""

        try:
            response = await self.llm_gateway.complete(
                prompt=prompt,
                model=self.planner_model,
                temperature=0.3
            )

            # Parse JSON response
            import json
            cleaned = response.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned.split("```")[1]
                if cleaned.startswith("json"):
                    cleaned = cleaned[4:]
            analysis_data = json.loads(cleaned)

            # Convert string keys to ints
            return {int(k): v for k, v in analysis_data.items()}

        except Exception as e:
            logger.warning(f"Lightweight analysis failed: {e}, using defaults")
            # Return conservative defaults
            return {
                i: {
                    "estimated_step_count": 5,
                    "complexity_score": "Medium",
                    "estimated_time_minutes": 10,
                    "estimated_cost_usd": Decimal("0.005"),
                    "risk_factors": ["Unknown - analysis failed"]
                }
                for i in range(len(proposals))
            }

    def _format_proposals_for_analysis(self, proposals: List[dict]) -> str:
        """Format proposals for lightweight analysis prompt"""
        formatted = []
        for i, p in enumerate(proposals, 1):
            formatted.append(f"""
Proposal {i}: {p.get('title', 'Untitled')}
Approach: {p.get('approach', 'N/A')}
Description: {p.get('description', 'N/A')}
""")
        return "\n".join(formatted)
```

**Step 3: Modify generate_strawman_proposals to use analysis**

Find the `generate_strawman_proposals` method and modify the return statement (around line 80):

```python
    async def generate_strawman_proposals(
        self,
        plan: Plan
    ) -> List[ProposalWithAnalysis]:
        """Generate strawman proposals with lightweight analysis."""

        # ... existing code to generate raw proposals ...

        # Generate raw proposals
        raw_proposals = await self._generate_proposals_raw(plan)

        # Add lightweight analysis
        analysis = await self._analyze_proposals_lightweight(plan, raw_proposals)

        # Merge and return
        proposals_with_analysis = []
        for i, raw_prop in enumerate(raw_proposals, 1):
            prop_data = {
                **raw_prop,
                "proposal_id": i,
                **analysis.get(i, {
                    "estimated_step_count": 5,
                    "complexity_score": "Medium",
                    "estimated_time_minutes": 10,
                    "estimated_cost_usd": Decimal("0.005"),
                    "risk_factors": []
                })
            }
            proposals_with_analysis.append(ProposalWithAnalysis(**prop_data))

        return proposals_with_analysis
```

**Step 4: Write test for lightweight analysis**

Create test file:

```bash
cat > tests/test_planner_comparison.py << 'EOF'
import pytest
from planweaver.models.plan import Plan, ProposalWithAnalysis
from decimal import Decimal

@pytest.mark.asyncio
async def test_lightweight_analysis_generates_estimates(planner_service):
    """Lightweight analysis should generate estimates for all proposals"""
    plan = Plan(
        session_id="test-123",
        user_intent="Add authentication to API"
    )

    proposals = await planner_service.generate_strawman_proposals(plan)

    assert len(proposals) >= 2
    for prop in proposals:
        assert prop.estimated_step_count > 0
        assert prop.complexity_score in ["Low", "Medium", "High"]
        assert prop.estimated_time_minutes > 0
        assert prop.estimated_cost_usd >= 0
        assert isinstance(prop.risk_factors, list)


@pytest.mark.asyncio
async def test_lightweight_analysis_fallback_on_error(planner_service):
    """Should return defaults if LLM analysis fails"""
    plan = Plan(session_id="test-456", user_intent="Test plan")

    # Mock LLM to fail
    planner_service.llm_gateway.complete = pytest.async_mock(side_effect=Exception("LLM failed"))

    proposals = await planner_service.generate_strawman_proposals(plan)

    # Should still return proposals with defaults
    assert len(proposals) >= 2
    for prop in proposals:
        assert prop.estimated_step_count == 5  # Default
        assert prop.complexity_score == "Medium"  # Default
EOF
```

**Step 5: Run tests to verify**

Run: `uv run pytest tests/test_planner_comparison.py::test_lightweight_analysis_generates_estimates -v`

Expected: FAIL (method not yet modified) or PASS (if already correct)

**Step 6: Fix any import errors**

If you see import errors, add at top of `planner.py`:

```python
from decimal import Decimal
```

**Step 7: Run tests again**

Run: `uv run pytest tests/test_planner_comparison.py -v`

Expected: PASS

**Step 8: Commit**

```bash
git add src/planweaver/services/planner.py tests/test_planner_comparison.py
git commit -m "feat: add lightweight proposal analysis with step/time/cost estimates"
```

---

## Task 3: Create Comparison Service

**Files:**
- Create: `src/planweaver/services/comparison_service.py`
- Test: `tests/test_comparison_service.py`

**Step 1: Create comparison service file**

```bash
cat > src/planweaver/services/comparison_service.py << 'EOF'
"""Service for comparing proposals with detailed execution graphs."""

from typing import Dict, List, Tuple
import asyncio
import logging
from decimal import Decimal

from planweaver.models.plan import (
    Plan,
    Proposal,
    ProposalDetail,
    ProposalComparison,
    StepSummary,
    ExecutionStep
)

logger = logging.getLogger(__name__)


class ProposalComparisonService:
    """Service for generating detailed proposal comparisons."""

    def __init__(self, planner, llm_gateway):
        self.planner = planner
        self.llm = llm_gateway
        self._graph_cache: Dict[Tuple[str, int], List[ExecutionStep]] = {}

    async def compare_proposals(
        self,
        plan: Plan,
        proposal_ids: List[int]
    ) -> ProposalComparison:
        """Generate detailed comparison of selected proposals.

        Args:
            plan: The plan containing proposals
            proposal_ids: List of proposal IDs to compare (must be >= 2)

        Returns:
            ProposalComparison with full execution graphs and diff analysis

        Raises:
            ValueError: If fewer than 2 proposals provided
        """
        if len(proposal_ids) < 2:
            raise ValueError(
                f"Comparison requires at least 2 proposals. Got {len(proposal_ids)}"
            )

        # Generate full execution graphs for each proposal
        proposal_details = []
        for prop_id in proposal_ids:
            try:
                graph = await self._generate_or_get_execution_graph(plan, prop_id)
                time_est = self._estimate_time(graph)
                cost_est = await self._estimate_cost(graph)
                risks = self._extract_risks(graph)

                proposal_details.append(ProposalDetail(
                    proposal_id=prop_id,
                    full_execution_graph=graph,
                    accurate_time_estimate=time_est,
                    accurate_cost_estimate=cost_est,
                    all_risk_factors=risks
                ))
            except Exception as e:
                logger.error(f"Failed to generate details for proposal {prop_id}: {e}")
                # Add partial result with error
                proposal_details.append(ProposalDetail(
                    proposal_id=prop_id,
                    full_execution_graph=[],
                    accurate_time_estimate=0,
                    accurate_cost_estimate=Decimal("0"),
                    all_risk_factors=[],
                    generation_error=str(e)
                ))

        # Compute diff between proposals
        common_steps = self._find_common_steps(proposal_details)
        unique_steps = self._find_unique_steps(proposal_details, common_steps)

        # Build comparison metrics
        time_comparison = {p.proposal_id: p.accurate_time_estimate
                          for p in proposal_details}
        cost_comparison = {p.proposal_id: p.accurate_cost_estimate
                          for p in proposal_details}
        complexity_comparison = {
            p.proposal_id: self._calculate_complexity_score(p)
            for p in proposal_details
        }

        return ProposalComparison(
            session_id=plan.session_id,
            proposals=proposal_details,
            common_steps=common_steps,
            unique_steps_by_proposal=unique_steps,
            time_comparison=time_comparison,
            cost_comparison=cost_comparison,
            complexity_comparison=complexity_comparison
        )

    async def _generate_or_get_execution_graph(
        self,
        plan: Plan,
        proposal_id: int
    ) -> List[ExecutionStep]:
        """Generate execution graph or retrieve from cache."""
        cache_key = (plan.session_id, proposal_id)

        # Check cache
        if cache_key in self._graph_cache:
            logger.debug(f"Cache hit for {cache_key}")
            return self._graph_cache[cache_key]

        # Generate graph
        logger.debug(f"Generating execution graph for proposal {proposal_id}")
        proposal = plan.get_proposal_by_id(proposal_id)

        # Use planner's decompose method
        # Note: You may need to add this method to Planner or Orchestrator
        graph = await self.planner.decompose_into_steps(plan, proposal)

        # Cache for 10 minutes
        self._graph_cache[cache_key] = graph
        asyncio.create_task(self._expire_cache(cache_key, 600))

        return graph

    async def _expire_cache(self, key: Tuple[str, int], delay: int):
        """Expire cache entry after delay seconds."""
        await asyncio.sleep(delay)
        if key in self._graph_cache:
            del self._graph_cache[key]
            logger.debug(f"Expired cache for {key}")

    def _find_common_steps(
        self,
        proposals: List[ProposalDetail]
    ) -> List[StepSummary]:
        """Find steps common to all proposals using fuzzy matching."""
        if not proposals:
            return []

        # Get all steps from first proposal
        first_steps = {s.task.lower(): s for s in proposals[0].full_execution_graph}

        common = []
        for step in proposals[0].full_execution_graph:
            # Check if similar step exists in all other proposals
            is_common = True
            for prop in proposals[1:]:
                if not self._has_similar_step(step, prop.full_execution_graph):
                    is_common = False
                    break

            if is_common:
                common.append(StepSummary(
                    task=step.task,
                    complexity=self._infer_step_complexity(step),
                    estimated_time_minutes=2  # Default
                ))

        return common

    def _has_similar_step(
        self,
        step: ExecutionStep,
        steps: List[ExecutionStep]
    ) -> bool:
        """Check if a similar step exists in the list."""
        step_lower = step.task.lower()

        for s in steps:
            # Direct match
            if s.task.lower() == step_lower:
                return True

            # Fuzzy match (if one contains the other)
            if (len(step_lower) > 5 and
                (step_lower in s.task.lower() or s.task.lower() in step_lower)):
                return True

        return False

    def _find_unique_steps(
        self,
        proposals: List[ProposalDetail],
        common_steps: List[StepSummary]
    ) -> Dict[int, List[StepSummary]]:
        """Find steps unique to each proposal."""
        common_tasks = {s.task.lower() for s in common_steps}

        unique_by_proposal = {}
        for prop in proposals:
            unique = []
            for step in prop.full_execution_graph:
                if step.task.lower() not in common_tasks:
                    unique.append(StepSummary(
                        task=step.task,
                        complexity=self._infer_step_complexity(step),
                        estimated_time_minutes=2
                    ))
            unique_by_proposal[prop.proposal_id] = unique

        return unique_by_proposal

    def _infer_step_complexity(self, step: ExecutionStep) -> str:
        """Infer complexity from step description."""
        task_lower = step.task.lower()

        # High complexity indicators
        high_keywords = ["migrate", "deploy", "architecture", "integration", "refactor"]
        if any(kw in task_lower for kw in high_keywords):
            return "High"

        # Low complexity indicators
        low_keywords = ["install", "update", "configure", "test", "verify", "backup"]
        if any(kw in task_lower for kw in low_keywords):
            return "Low"

        return "Medium"

    def _calculate_complexity_score(self, prop: ProposalDetail) -> str:
        """Calculate overall complexity score for proposal."""
        if not prop.full_execution_graph:
            return "Medium"

        complexities = [self._infer_step_complexity(s) for s in prop.full_execution_graph]

        high_count = complexities.count("High")
        if high_count >= len(complexities) / 2:
            return "High"

        low_count = complexities.count("Low")
        if low_count >= len(complexities) / 2:
            return "Low"

        return "Medium"

    def _estimate_time(self, steps: List[ExecutionStep]) -> int:
        """Estimate execution time in minutes."""
        base_time_per_step = 2  # minutes
        complexity_multiplier = {"Low": 1.0, "Medium": 1.5, "High": 2.5}

        total = 0
        for step in steps:
            complexity = self._infer_step_complexity(step)
            total += base_time_per_step * complexity_multiplier[complexity]

        return int(total)

    async def _estimate_cost(self, steps: List[ExecutionStep]) -> Decimal:
        """Estimate execution cost in USD."""
        if not steps:
            return Decimal("0")

        tokens_per_step = 500
        pricing = {
            "gemini-2.5-flash": 0.075,  # $0.075 per 1M tokens
            "deepseek-chat": 0.14,
            "claude-3.5-sonnet": 3.0,
            "gpt-4o": 2.5,
        }

        model = steps[0].assigned_model if steps else "gemini-2.5-flash"
        price_per_m = pricing.get(model, 0.15)

        total_tokens = len(steps) * tokens_per_step
        cost = (total_tokens / 1_000_000) * price_per_m

        return Decimal(str(round(cost, 4)))

    def _extract_risks(self, steps: List[ExecutionStep]) -> List[str]:
        """Extract risk factors from execution steps."""
        risks = []

        risk_keywords = {
            "production": "Production changes",
            "delete": "Data deletion risk",
            "migrate": "Data migration risk",
            "external": "External API dependency",
            "third-party": "Third-party service dependency",
        }

        for step in steps:
            task_lower = step.task.lower()
            for keyword, risk in risk_keywords.items():
                if keyword in task_lower and risk not in risks:
                    risks.append(risk)

        return risks[:5]  # Max 5 risks
EOF
```

**Step 2: Add get_proposal_by_id method to Plan model if needed**

Check if method exists: `grep -n "def get_proposal_by_id" src/planweaver/models/plan.py`

If not found, add to Plan class:

```python
def get_proposal_by_id(self, proposal_id: int) -> Proposal:
    """Get proposal by ID."""
    for prop in self.proposals:
        if prop.id == proposal_id:
            return prop
    raise ValueError(f"Proposal {proposal_id} not found")
```

**Step 3: Write tests for comparison service**

```bash
cat > tests/test_comparison_service.py << 'EOF'
import pytest
from planweaver.services.comparison_service import ProposalComparisonService
from planweaver.models.plan import Plan, Proposal, ExecutionStep
from decimal import Decimal

@pytest.fixture
def comparison_service(planner_service, llm_gateway):
    return ProposalComparisonService(planner_service, llm_gateway)


@pytest.mark.asyncio
async def test_compare_proposals_requires_at_least_two(comparison_service):
    """Should raise error if fewer than 2 proposals"""
    plan = Plan(session_id="test", user_intent="Test")

    with pytest.raises(ValueError, match="at least 2 proposals"):
        await comparison_service.compare_proposals(plan, [1])


@pytest.mark.asyncio
async def test_compare_proposals_finds_common_steps(comparison_service):
    """Should identify steps common to all proposals"""
    # Create test plan with mock proposals
    plan = Plan(session_id="test", user_intent="Test")

    # This test will need mock setup - implement based on your actual Plan structure
    # For now, test the logic with mock data

    proposals = [
        create_mock_proposal_detail(1, ["Step A", "Step B", "Step C"]),
        create_mock_proposal_detail(2, ["Step A", "Step B", "Step D"])
    ]

    # Mock the graph generation
    comparison_service._generate_or_get_execution_graph = pytest.async_mock(
        return_value=proposals[0].full_execution_graph
    )

    result = await comparison_service.compare_proposals(plan, [1, 2])

    assert len(result.common_steps) == 2  # Step A and Step B
    assert len(result.unique_steps_by_proposal[1]) == 1  # Step C
    assert len(result.unique_steps_by_proposal[2]) == 1  # Step D


@pytest.mark.asyncio
async def test_estimate_time_weights_complexity(comparison_service):
    """Complex steps should take longer"""
    simple_steps = [create_mock_step("Install package", complexity="Low")]
    complex_steps = [create_mock_step("Migrate database", complexity="High")]

    simple_time = comparison_service._estimate_time(simple_steps)
    complex_time = comparison_service._estimate_time(complex_steps)

    assert complex_time > simple_time


def create_mock_proposal_detail(prop_id, step_tasks):
    """Helper to create mock ProposalDetail"""
    from planweaver.models.plan import ProposalDetail

    steps = [create_mock_step(task) for task in step_tasks]

    return ProposalDetail(
        proposal_id=prop_id,
        full_execution_graph=steps,
        accurate_time_estimate=len(steps) * 2,
        accurate_cost_estimate=Decimal("0.01"),
        all_risk_factors=[]
    )


def create_mock_step(task, complexity="Medium"):
    """Helper to create mock ExecutionStep"""
    return ExecutionStep(
        step_id=f"step-{task}",
        task=task,
        prompt_template_id="default",
        assigned_model="gemini-2.5-flash",
        dependencies=[]
    )
EOF
```

**Step 4: Run tests**

Run: `uv run pytest tests/test_comparison_service.py -v`

Expected: Some tests may fail until you integrate with actual Plan/Proposal structure

**Step 5: Fix any issues based on your actual model structure**

Adapt the service to work with your existing Plan and Proposal models. Key things to check:
- How proposals are stored in Plan
- Method to get proposal by ID
- ExecutionStep structure

**Step 6: Run tests again**

Run: `uv run pytest tests/test_comparison_service.py -v`

Expected: PASS

**Step 7: Commit**

```bash
git add src/planweaver/services/comparison_service.py tests/test_comparison_service.py
git commit -m "feat: add ProposalComparisonService for detailed proposal comparison"
```

---

## Task 4: Add Comparison API Endpoint

**Files:**
- Modify: `src/planweaver/api/routers/sessions.py`
- Modify: `src/planweaver/api/main.py` (for dependency injection)

**Step 1: Read current sessions router**

Run: `cat src/planweaver/api/routers/sessions.py`

**Step 2: Add dependency for comparison service**

Add to router imports and dependencies (after existing dependencies):

```python
from planweaver.services.comparison_service import ProposalComparisonService

async def get_comparison_service(
    comparison_service: ProposalComparisonService = Depends(get_comparison_service)
) -> ProposalComparisonService:
    """Dependency injection for comparison service"""
    return comparison_service
```

Note: You may need to add this to `main.py` instead if using dependency injection pattern there.

**Step 3: Add comparison endpoint**

Add to sessions router:

```python
@router.post("/{session_id}/compare-proposals", response_model=ProposalComparison)
async def compare_proposals(
    session_id: str,
    request: ComparisonRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator),
    comparison_service: ProposalComparisonService = Depends(get_comparison_service)
):
    """Compare detailed execution graphs for selected proposals.

    Args:
        session_id: Session identifier
        request: Comparison request with proposal_ids to compare

    Returns:
        ProposalComparison with full execution graphs and diff analysis

    Raises:
        HTTPException 404: If proposal IDs are invalid
        HTTPException 400: If fewer than 2 proposals provided
    """
    plan = await orchestrator.get_plan(session_id)

    # Validate proposal IDs
    valid_ids = {p.id for p in plan.proposals}
    invalid_ids = set(request.proposal_ids) - valid_ids

    if invalid_ids:
        raise HTTPException(
            status_code=404,
            detail=f"Proposals not found: {invalid_ids}. Valid IDs: {valid_ids}"
        )

    if len(request.proposal_ids) < 2:
        raise HTTPException(
            status_code=400,
            detail=f"Comparison requires at least 2 proposals. Got {len(request.proposal_ids)}"
        )

    try:
        comparison = await comparison_service.compare_proposals(
            plan=plan,
            proposal_ids=request.proposal_ids
        )
        return comparison
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Comparison failed: {e}")
        raise HTTPException(
            status_code=500,
            detail="Unable to generate comparison. Please try again."
        )
```

**Step 4: Add imports to router**

Ensure these are imported at top of file:

```python
from planweaver.models.plan import ComparisonRequest, ProposalComparison
from fastapi import HTTPException
```

**Step 5: Test API endpoint manually**

Start server: `uv run uvicorn planweaver.api.main:app --reload`

Test endpoint: `curl -X POST http://localhost:8000/api/v1/sessions/test-session/compare-proposals -H "Content-Type: application/json" -d '{"proposal_ids": [1, 2]}'`

Expected: Either 404 (session not found) or 400 (validation), but endpoint should exist

**Step 6: Write integration test**

```bash
cat > tests/integration/test_comparison_api.py << 'EOF'
import pytest
from fastapi.testclient import TestClient

@pytest.mark.asyncio
async def test_compare_proposals_endpoint(client, test_session):
    """Test comparison endpoint returns valid response"""
    # Create session with proposals
    response = client.post(
        f"/api/v1/sessions/{test_session.id}/compare-proposals",
        json={"proposal_ids": [1, 2]}
    )

    assert response.status_code == 200

    data = response.json()
    assert "session_id" in data
    assert "proposals" in data
    assert "common_steps" in data
    assert len(data["proposals"]) == 2


@pytest.mark.asyncio
async def test_compare_proposals_validates_ids(client, test_session):
    """Should return 404 for invalid proposal IDs"""
    response = client.post(
        f"/api/v1/sessions/{test_session.id}/compare-proposals",
        json={"proposal_ids": [99, 100]}
    )

    assert response.status_code == 404


@pytest.mark.asyncio
async def test_compare_proposals_requires_minimum_two(client, test_session):
    """Should return 400 for single proposal"""
    response = client.post(
        f"/api/v1/sessions/{test_session.id}/compare-proposals",
        json={"proposal_ids": [1]}
    )

    assert response.status_code == 400
    assert "at least 2 proposals" in response.json()["detail"]
EOF
```

**Step 7: Run integration tests**

Run: `uv run pytest tests/integration/test_comparison_api.py -v`

Expected: PASS (may need to adjust based on your test setup)

**Step 8: Commit**

```bash
git add src/planweaver/api/routers/sessions.py tests/integration/test_comparison_api.py
git commit -m "feat: add compare-proposals API endpoint"
```

---

## Task 5: Create Frontend Comparison View Component

**Files:**
- Create: `frontend/src/components/ProposalComparisonView.tsx`
- Create: `frontend/src/components/DiffComparison.tsx`
- Create: `frontend/src/components/SideBySideComparison.tsx`
- Create: `frontend/src/types/comparison.ts`

**Step 1: Create TypeScript types**

```bash
mkdir -p frontend/src/types
cat > frontend/src/types/comparison.ts << 'EOF'
export interface StepSummary {
  task: string;
  complexity: 'Low' | 'Medium' | 'High';
  estimated_time_minutes: number;
}

export interface ProposalDetail {
  proposal_id: number;
  full_execution_graph: ExecutionStep[];
  accurate_time_estimate: number;
  accurate_cost_estimate: number;
  all_risk_factors: string[];
  generation_error?: string;
}

export interface ProposalComparison {
  session_id: string;
  proposals: ProposalDetail[];
  common_steps: StepSummary[];
  unique_steps_by_proposal: Record<number, StepSummary[]>;
  time_comparison: Record<number, number>;
  cost_comparison: Record<number, number>;
  complexity_comparison: Record<number, string>;
}

export interface ComparisonRequest {
  proposal_ids: number[];
}

export interface ProposalWithAnalysis {
  proposal_id: number;
  title: string;
  description: string;
  approach: string;
  pros: string[];
  cons: string[];
  estimated_step_count: number;
  complexity_score: 'Low' | 'Medium' | 'High';
  estimated_time_minutes: number;
  estimated_cost_usd: number;
  risk_factors: string[];
}
EOF
```

**Step 2: Create API client for comparison**

```bash
cat > frontend/src/api/comparison.ts << 'EOF'
import axios from 'axios';
import type { ProposalComparison, ComparisonRequest } from '../types/comparison';

const API_BASE = import.meta.env.VITE_API_BASE || 'http://localhost:8000/api/v1';

export async function compareProposals(
  sessionId: string,
  proposalIds: number[]
): Promise<ProposalComparison> {
  const response = await axios.post<ProposalComparison>(
    `${API_BASE}/sessions/${sessionId}/compare-proposals`,
    { proposal_ids: proposalIds }
  );
  return response.data;
}
EOF
```

**Step 3: Create main comparison view component**

```bash
cat > frontend/src/components/ProposalComparisonView.tsx << 'EOF'
import React, { useState } from 'react';
import { compareProposals } from '../api/comparison';
import type { ProposalComparison, ProposalWithAnalysis } from '../types/comparison';
import { DiffComparison } from './DiffComparison';
import { SideBySideComparison } from './SideBySideComparison';
import { ProposalSelector } from './ProposalSelector';

interface ProposalComparisonViewProps {
  sessionId: string;
  proposals: ProposalWithAnalysis[];
  onClose: () => void;
  onSelectProposal: (proposalId: number) => void;
}

export function ProposalComparisonView({
  sessionId,
  proposals,
  onClose,
  onSelectProposal,
}: ProposalComparisonViewProps) {
  const [comparison, setComparison] = useState<ProposalComparison | null>(null);
  const [isExpanded, setIsExpanded] = useState(false);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [selectedIds, setSelectedIds] = useState<number[]>([]);

  const handleCompare = async (proposalIds: number[]) => {
    setError(null);
    setLoading(true);
    setSelectedIds(proposalIds);

    try {
      const result = await compareProposals(sessionId, proposalIds);
      setComparison(result);
    } catch (err: any) {
      setError(err.response?.data?.detail || 'Failed to load comparison');
    } finally {
      setLoading(false);
    }
  };

  const handleSelectProposal = (proposalId: number) => {
    onSelectProposal(proposalId);
    onClose();
  };

  return (
    <div className="comparison-modal">
      <div className="comparison-header">
        <h2>Compare Proposals</h2>
        <button onClick={onClose} className="close-button">‚úï</button>
      </div>

      {error && (
        <div className="error-message">
          {error}
        </div>
      )}

      {loading && (
        <div className="loading-spinner">
          Loading comparison...
        </div>
      )}

      {!comparison ? (
        <ProposalSelector
          proposals={proposals}
          onCompare={handleCompare}
        />
      ) : isExpanded ? (
        <SideBySideComparison
          comparison={comparison}
          onCollapse={() => setIsExpanded(false)}
          onSelectProposal={handleSelectProposal}
        />
      ) : (
        <DiffComparison
          comparison={comparison}
          onExpand={() => setIsExpanded(true)}
          onSelectProposal={handleSelectProposal}
        />
      )}
    </div>
  );
}
EOF
```

**Step 4: Create proposal selector component**

```bash
cat > frontend/src/components/ProposalSelector.tsx << 'EOF'
import React, { useState } from 'react';
import type { ProposalWithAnalysis } from '../types/comparison';

interface ProposalSelectorProps {
  proposals: ProposalWithAnalysis[];
  onCompare: (proposalIds: number[]) => void;
}

export function ProposalSelector({ proposals, onCompare }: ProposalSelectorProps) {
  const [selected, setSelected] = useState<number[]>([]);

  const toggleProposal = (id: number) => {
    if (selected.includes(id)) {
      setSelected(selected.filter((s) => s !== id));
    } else if (selected.length < 3) {
      setSelected([...selected, id]);
    }
  };

  const handleCompare = () => {
    if (selected.length >= 2) {
      onCompare(selected);
    }
  };

  return (
    <div className="proposal-selector">
      <h3>Select proposals to compare (2-3)</h3>

      <div className="proposal-list">
        {proposals.map((proposal) => (
          <div
            key={proposal.proposal_id}
            className={`proposal-card ${selected.includes(proposal.proposal_id) ? 'selected' : ''}`}
            onClick={() => toggleProposal(proposal.proposal_id)}
          >
            <h4>{proposal.title}</h4>
            <p>{proposal.description}</p>
            <div className="quick-metrics">
              <span>‚è± {proposal.estimated_time_minutes}m</span>
              <span>üí∞ ${proposal.estimated_cost_usd}</span>
              <span>‚ö†Ô∏è {proposal.complexity_score}</span>
            </div>
          </div>
        ))}
      </div>

      <button
        className="compare-button"
        onClick={handleCompare}
        disabled={selected.length < 2}
      >
        Compare Selected ({selected.length}/2-3)
      </button>
    </div>
  );
}
EOF
```

**Step 5: Create diff comparison component**

```bash
cat > frontend/src/components/DiffComparison.tsx << 'EOF'
import React from 'react';
import type { ProposalComparison } from '../types/comparison';

interface DiffComparisonProps {
  comparison: ProposalComparison;
  onExpand: () => void;
  onSelectProposal: (proposalId: number) => void;
}

export function DiffComparison({ comparison, onExpand, onSelectProposal }: DiffComparisonProps) {
  const proposals = comparison.proposals;

  return (
    <div className="diff-comparison">
      <div className="comparison-actions">
        <button onClick={onExpand} className="expand-button">
          Show All Side-by-Side ‚ñº
        </button>
      </div>

      {/* Quick metrics cards */}
      <div className="quick-metrics">
        {proposals.map((p) => (
          <div key={p.proposal_id} className="metric-card">
            <h4>Proposal {p.proposal_id}</h4>
            <div className="metrics">
              <div>‚è± {comparison.time_comparison[p.proposal_id]}m</div>
              <div>üí∞ ${comparison.cost_comparison[p.proposal_id]}</div>
              <div>‚ö†Ô∏è {comparison.complexity_comparison[p.proposal_id]}</div>
            </div>
            <button onClick={() => onSelectProposal(p.proposal_id)}>
              Select This
            </button>
          </div>
        ))}
      </div>

      {/* Common steps */}
      {comparison.common_steps.length > 0 && (
        <div className="step-section common-steps">
          <h3>üìä Common Steps ({comparison.common_steps.length})</h3>
          {comparison.common_steps.map((step, i) => (
            <div key={i} className="step-item">
              ‚úì {step.task}
            </div>
          ))}
        </div>
      )}

      {/* Unique steps per proposal */}
      {Object.entries(comparison.unique_steps_by_proposal).map(([propId, steps]) => (
        <div key={propId} className="step-section unique-steps">
          <h3>üîµ Unique to Proposal {propId}</h3>
          {steps.map((step, i) => (
            <div key={i} className="step-item">
              ‚Üí {step.task}
            </div>
          ))}
        </div>
      ))}
    </div>
  );
}
EOF
```

**Step 6: Create side-by-side comparison component**

```bash
cat > frontend/src/components/SideBySideComparison.tsx << 'EOF'
import React from 'react';
import type { ProposalComparison } from '../types/comparison';

interface SideBySideComparisonProps {
  comparison: ProposalComparison;
  onCollapse: () => void;
  onSelectProposal: (proposalId: number) => void;
}

export function SideBySideComparison({ comparison, onCollapse, onSelectProposal }: SideBySideComparisonProps) {
  return (
    <div className="side-by-side-comparison">
      <div className="comparison-actions">
        <button onClick={onCollapse} className="collapse-button">
          ‚ñ≤ Show Differences Only
        </button>
      </div>

      <div className="proposals-grid">
        {comparison.proposals.map((proposal) => (
          <div key={proposal.proposal_id} className="proposal-detail">
            <h3>üîµ Proposal {proposal.proposal_id}</h3>

            <div className="metrics">
              <div>‚è± {comparison.time_comparison[proposal.proposal_id]} minutes</div>
              <div>üí∞ ${comparison.cost_comparison[proposal.proposal_id]}</div>
              <div>‚ö†Ô∏è {comparison.complexity_comparison[proposal.proposal_id]}</div>
            </div>

            <div className="execution-steps">
              <h4>üìã Execution Steps ({proposal.full_execution_graph.length})</h4>
              {proposal.full_execution_graph.map((step, i) => (
                <div key={i} className="step-item">
                  {step.task}
                </div>
              ))}
            </div>

            {proposal.all_risk_factors.length > 0 && (
              <div className="risk-factors">
                <h4>üí° Risk Factors</h4>
                {proposal.all_risk_factors.map((risk, i) => (
                  <div key={i} className="risk-item">‚Ä¢ {risk}</div>
                ))}
              </div>
            )}

            <button
              className="select-button"
              onClick={() => onSelectProposal(proposal.proposal_id)}
            >
              Select This Proposal
            </button>
          </div>
        ))}
      </div>
    </div>
  );
}
EOF
```

**Step 7: Add CSS styles**

```bash
cat > frontend/src/components/ProposalComparisonView.css << 'EOF'
.comparison-modal {
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  background: white;
  border-radius: 8px;
  box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
  max-width: 1200px;
  width: 90%;
  max-height: 90vh;
  overflow-y: auto;
  padding: 20px;
  z-index: 1000;
}

.comparison-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 20px;
}

.comparison-header h2 {
  margin: 0;
}

.close-button {
  background: none;
  border: none;
  font-size: 24px;
  cursor: pointer;
}

.proposal-selector {
  margin: 20px 0;
}

.proposal-list {
  display: grid;
  gap: 15px;
  margin: 20px 0;
}

.proposal-card {
  border: 2px solid #ddd;
  border-radius: 8px;
  padding: 15px;
  cursor: pointer;
  transition: border-color 0.2s;
}

.proposal-card:hover {
  border-color: #4a90e2;
}

.proposal-card.selected {
  border-color: #4a90e2;
  background: #f0f7ff;
}

.quick-metrics {
  display: flex;
  gap: 20px;
  margin-top: 10px;
}

.metric-card {
  border: 1px solid #ddd;
  border-radius: 8px;
  padding: 15px;
  text-align: center;
}

.compare-button {
  background: #4a90e2;
  color: white;
  border: none;
  border-radius: 4px;
  padding: 10px 20px;
  font-size: 16px;
  cursor: pointer;
}

.compare-button:disabled {
  background: #ccc;
  cursor: not-allowed;
}

.diff-comparison,
.side-by-side-comparison {
  margin: 20px 0;
}

.comparison-actions {
  text-align: center;
  margin-bottom: 20px;
}

.expand-button,
.collapse-button {
  background: #f0f0f0;
  border: 1px solid #ddd;
  border-radius: 4px;
  padding: 8px 16px;
  cursor: pointer;
}

.step-section {
  margin: 20px 0;
  padding: 15px;
  background: #f9f9f9;
  border-radius: 8px;
}

.step-item {
  padding: 5px 0;
}

.proposals-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
  gap: 20px;
  margin-top: 20px;
}

.proposal-detail {
  border: 1px solid #ddd;
  border-radius: 8px;
  padding: 20px;
}

.metrics {
  display: flex;
  gap: 15px;
  margin: 15px 0;
  padding: 10px;
  background: #f0f0f0;
  border-radius: 4px;
}

.execution-steps,
.risk-factors {
  margin: 15px 0;
}

.select-button {
  background: #4a90e2;
  color: white;
  border: none;
  border-radius: 4px;
  padding: 10px 20px;
  cursor: pointer;
  width: 100%;
  margin-top: 15px;
}

.error-message {
  background: #fee;
  color: #c33;
  padding: 10px;
  border-radius: 4px;
  margin-bottom: 15px;
}

.loading-spinner {
  text-align: center;
  padding: 40px;
}
EOF
```

**Step 8: Import CSS in main component**

Add to ProposalComparisonView.tsx imports:

```typescript
import './ProposalComparisonView.css';
```

**Step 9: Commit**

```bash
git add frontend/src/components/ frontend/src/types/ frontend/src/api/
git commit -m "feat: add frontend comparison components (diff and side-by-side views)"
```

---

## Task 6: Integrate Comparison into PlanView

**Files:**
- Modify: `frontend/src/components/PlanView.tsx`

**Step 1: Read current PlanView**

Run: `cat frontend/src/components/PlanView.tsx`

**Step 2: Add comparison state and handler**

Add to component (before return statement):

```typescript
import { useState } from 'react';
import { ProposalComparisonView } from './ProposalComparisonView';
import type { ProposalWithAnalysis } from '../types/comparison';

// In component
function PlanView({ session, onSelectProposal, ...props }) {
  const [showComparison, setShowComparison] = useState(false);

  const handleCloseComparison = () => {
    setShowComparison(false);
  };

  const handleSelectFromComparison = (proposalId: number) => {
    onSelectProposal(proposalId);
    setShowComparison(false);
  };

  // ... existing code ...
```

**Step 3: Add compare button in proposals list**

Add where proposals are displayed (check for proposal list/loop):

```typescript
{/* After proposal list */}
{session.proposals && session.proposals.length >= 2 && (
  <div className="compare-section">
    <button
      onClick={() => setShowComparison(true)}
      className="compare-all-button"
    >
      Compare Proposals ({session.proposals.length} available)
    </button>
  </div>
)}

{/* Comparison modal */}
{showComparison && (
  <ProposalComparisonView
    sessionId={session.id}
    proposals={session.proposals}
    onClose={handleCloseComparison}
    onSelectProposal={handleSelectFromComparison}
  />
)}
```

**Step 4: Add CSS for compare button**

```bash
cat >> frontend/src/components/PlanView.css << 'EOF'

.compare-section {
  margin: 20px 0;
  text-align: center;
}

.compare-all-button {
  background: #6c5ce7;
  color: white;
  border: none;
  border-radius: 6px;
  padding: 12px 24px;
  font-size: 16px;
  cursor: pointer;
  transition: background 0.2s;
}

.compare-all-button:hover {
  background: #5b4cdb;
}
EOF
```

**Step 5: Verify TypeScript compiles**

Run: `cd frontend && npm run type-check`

Expected: No errors

**Step 6: Commit**

```bash
git add frontend/src/components/PlanView.tsx frontend/src/components/PlanView.css
git commit -m "feat: integrate comparison view into PlanView with compare button"
```

---

## Task 7: Add Error Handling and Edge Cases

**Files:**
- Modify: `frontend/src/components/ProposalComparisonView.tsx`
- Modify: `src/planweaver/services/comparison_service.py`

**Step 1: Add better error messages to frontend**

Update ProposalComparisonView.tsx error handling:

```typescript
const getErrorMessage = (error: any): string => {
  if (error.response?.status === 404) {
    return 'Invalid proposal selection. Please choose valid proposals.';
  }
  if (error.response?.status === 400) {
    return error.response.data.detail || 'Invalid request';
  }
  if (error.response?.status === 500) {
    return 'Server error. Please try again.';
  }
  return 'Failed to load comparison. Please try again.';
};

// In catch block
catch (err: any) {
  setError(getErrorMessage(err));
} finally {
  setLoading(false);
}
```

**Step 2: Add loading skeleton**

Add loading component:

```bash
cat > frontend/src/components/ComparisonLoading.tsx << 'EOF'
import React from 'react';

export function ComparisonLoading() {
  return (
    <div className="comparison-loading">
      <div className="spinner"></div>
      <p>Generating comparison...</p>
      <small>This may take a few seconds</small>
    </div>
  );
}
EOF
```

Add CSS:

```bash
cat >> frontend/src/components/ProposalComparisonView.css << 'EOF'

.comparison-loading {
  text-align: center;
  padding: 60px 20px;
}

.spinner {
  border: 4px solid #f3f3f3;
  border-top: 4px solid #4a90e2;
  border-radius: 50%;
  width: 40px;
  height: 40px;
  animation: spin 1s linear infinite;
  margin: 0 auto 20px;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}
EOF
```

**Step 3: Add partial failure handling**

Update DiffComparison to handle partial failures:

```typescript
{comparison.proposals.some(p => p.generation_error) && (
  <div className="warning-message">
    ‚ö†Ô∏è Some details unavailable. Showing available information.
  </div>
)}
```

**Step 4: Add backend fallback for unknown models**

Update comparison_service.py:

```python
async def _estimate_cost(self, steps: List[ExecutionStep]) -> Decimal:
    """Estimate execution cost in USD."""
    if not steps:
        return Decimal("0")

    tokens_per_step = 500

    # Pricing database (prices per 1M tokens)
    pricing = {
        "gemini-2.5-flash": 0.075,
        "gemini-2.5-pro": 0.15,
        "deepseek-chat": 0.14,
        "deepseek-coder": 0.18,
        "claude-3.5-sonnet": 3.0,
        "claude-3-opus": 15.0,
        "gpt-4o": 2.5,
        "gpt-4o-mini": 0.15,
        "gpt-3.5-turbo": 0.5,
    }

    model = steps[0].assigned_model if steps else "gemini-2.5-flash"

    # Try exact match
    if model in pricing:
        price_per_m = pricing[model]
    # Try prefix match (e.g., "gemini-" -> 0.15)
    else:
        price_per_m = next(
            (v for k, v in pricing.items() if model.startswith(k)),
            0.15  # Default conservative estimate
        )

    total_tokens = len(steps) * tokens_per_step
    cost = (total_tokens / 1_000_000) * price_per_m

    return Decimal(str(round(cost, 4)))
```

**Step 5: Add cache clearing for development**

Add to comparison service:

```python
def clear_cache(self):
    """Clear all cached execution graphs."""
    self._graph_cache.clear()
    logger.info("Cleared comparison cache")
```

**Step 6: Write tests for edge cases**

```bash
cat >> tests/test_comparison_service.py << 'EOF'

@pytest.mark.asyncio
async def test_estimate_cost_unknown_model_uses_default(comparison_service):
    """Should use default pricing for unknown models"""
    steps = [create_mock_step("Test step", complexity="Low")]
    steps[0].assigned_model = "unknown-model-xyz"

    cost = await comparison_service._estimate_cost(steps)

    # Should not raise error, should use default
    assert cost >= 0


@pytest.mark.asyncio
async def test_compare_proposals_handles_partial_failures(comparison_service, plan):
    """Should return partial results if one proposal fails"""
    # Mock one proposal to fail
    async def mock_generate(plan, prop_id):
        if prop_id == 1:
            raise Exception("Failed")
        return [create_mock_step("Success")]

    comparison_service._generate_or_get_execution_graph = mock_generate

    result = await comparison_service.compare_proposals(plan, [1, 2])

    # Should still return comparison with error indicator
    assert len(result.proposals) == 2
    assert result.proposals[0].generation_error is not None
    assert result.proposals[1].generation_error is None
EOF
```

**Step 7: Run tests**

Run: `uv run pytest tests/test_comparison_service.py -v`

Expected: PASS

**Step 8: Commit**

```bash
git add frontend/src/components/ src/planweaver/services/comparison_service.py tests/
git commit -m "feat: add comprehensive error handling and edge case support"
```

---

## Task 8: Add Comprehensive Tests

**Files:**
- Test: `tests/test_comparison_service.py`
- Test: `frontend/src/components/ProposalComparisonView.test.tsx`

**Step 1: Complete backend test coverage**

```bash
cat >> tests/test_comparison_service.py << 'EOF'

@pytest.mark.asyncio
async def test_cache_expiration_after_delay(comparison_service):
    """Cache should expire after specified delay"""
    plan = Plan(session_id="test", user_intent="Test")

    # Add to cache
    comparison_service._graph_cache[("test", 1)] = [create_mock_step("Test")]

    # Wait for expiration (use short delay for test)
    await comparison_service._expire_cache(("test", 1), 0.1)
    await asyncio.sleep(0.2)

    assert ("test", 1) not in comparison_service._graph_cache


@pytest.mark.asyncio
async def test_find_common_steps_no_proposals(comparison_service):
    """Should return empty list if no proposals"""
    result = comparison_service._find_common_steps([])
    assert result == []


@pytest.mark.asyncio
async def test_estimate_time_no_steps(comparison_service):
    """Should return 0 for empty steps"""
    result = comparison_service._estimate_time([])
    assert result == 0


@pytest.mark.asyncio
async def test_extract_risks_identifies_keywords(comparison_service):
    """Should identify risk keywords in steps"""
    steps = [
        create_mock_step("Deploy to production server"),
        create_mock_step("Migrate user database"),
        create_mock_step("Call external API"),
    ]

    risks = comparison_service._extract_risks(steps)

    assert len(risks) > 0
    assert any("production" in r.lower() for r in risks)
    assert any("migration" in r.lower() for r in risks)


@pytest.mark.asyncio
async def test_infer_complexity_from_keywords(comparison_service):
    """Complexity inference based on keywords"""
    assert comparison_service._infer_step_complexity(
        create_mock_step("Migrate database architecture")
    ) == "High"

    assert comparison_service._infer_step_complexity(
        create_mock_step("Install package")
    ) == "Low"

    assert comparison_service._infer_step_complexity(
        create_mock_step("Update configuration file")
    ) == "Medium"
EOF
```

**Step 2: Add frontend component tests**

```bash
cat > frontend/src/components/ProposalComparisonView.test.tsx << 'EOF'
import { describe, it, expect, vi, beforeEach } from 'vitest';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { ProposalComparisonView } from './ProposalComparisonView';
import * as api from '../api/comparison';
import type { ProposalWithAnalysis, ProposalComparison } from '../types/comparison';

const mockProposals: ProposalWithAnalysis[] = [
  {
    proposal_id: 1,
    title: "Proposal 1",
    description: "First proposal",
    approach: "Incremental",
    pros: ["Safe"],
    cons: ["Slow"],
    estimated_step_count: 5,
    complexity_score: "Low",
    estimated_time_minutes: 10,
    estimated_cost_usd: 0.01,
    risk_factors: [],
  },
  {
    proposal_id: 2,
    title: "Proposal 2",
    description: "Second proposal",
    approach: "Big Bang",
    pros: ["Fast"],
    cons: ["Risky"],
    estimated_step_count: 3,
    complexity_score: "Medium",
    estimated_time_minutes: 6,
    estimated_cost_usd: 0.008,
    risk_factors: [],
  },
];

const mockComparison: ProposalComparison = {
  session_id: "test-123",
  proposals: [
    {
      proposal_id: 1,
      full_execution_graph: [],
      accurate_time_estimate: 10,
      accurate_cost_estimate: 0.01,
      all_risk_factors: [],
    },
    {
      proposal_id: 2,
      full_execution_graph: [],
      accurate_time_estimate: 6,
      accurate_cost_estimate: 0.008,
      all_risk_factors: [],
    },
  ],
  common_steps: [],
  unique_steps_by_proposal: { 1: [], 2: [] },
  time_comparison: { 1: 10, 2: 6 },
  cost_comparison: { 1: 0.01, 2: 0.008 },
  complexity_comparison: { 1: "Low", 2: "Medium" },
};

describe('ProposalComparisonView', () => {
  beforeEach(() => {
    vi.clearAllMocks();
  });

  it('renders proposal selector when no comparison', () => {
    render(
      <ProposalComparisonView
        sessionId="test-123"
        proposals={mockProposals}
        onClose={vi.fn()}
        onSelectProposal={vi.fn()}
      />
    );

    expect(screen.getByText('Select proposals to compare')).toBeInTheDocument();
  });

  it('shows compare button disabled when fewer than 2 selected', () => {
    render(
      <ProposalComparisonView
        sessionId="test-123"
        proposals={mockProposals}
        onClose={vi.fn()}
        onSelectProposal={vi.fn()}
      />
    );

    const button = screen.getByRole('button', { name: /compare selected/i });
    expect(button).toBeDisabled();
  });

  it('calls compare API when 2 proposals selected', async () => {
    const spy = vi.spyOn(api, 'compareProposals').mockResolvedValue(mockComparison);

    render(
      <ProposalComparisonView
        sessionId="test-123"
        proposals={mockProposals}
        onClose={vi.fn()}
        onSelectProposal={vi.fn()}
      />
    );

    // Select first proposal
    fireEvent.click(screen.getByText('Proposal 1'));
    // Select second proposal
    fireEvent.click(screen.getByText('Proposal 2'));
    // Click compare
    fireEvent.click(screen.getByRole('button', { name: /compare selected/i }));

    await waitFor(() => {
      expect(spy).toHaveBeenCalledWith('test-123', [1, 2]);
    });
  });

  it('displays diff comparison after loading', async () => {
    vi.spyOn(api, 'compareProposals').mockResolvedValue(mockComparison);

    render(
      <ProposalComparisonView
        sessionId="test-123"
        proposals={mockProposals}
        onClose={vi.fn()}
        onSelectProposal={vi.fn()}
      />
    );

    // Select and compare
    fireEvent.click(screen.getByText('Proposal 1'));
    fireEvent.click(screen.getByText('Proposal 2'));
    fireEvent.click(screen.getByRole('button', { name: /compare selected/i }));

    await waitFor(() => {
      expect(screen.getByText(/show all side-by-side/i)).toBeInTheDocument();
    });
  });

  it('switches to side-by-side view when expanded', async () => {
    vi.spyOn(api, 'compareProposals').mockResolvedValue(mockComparison);

    render(
      <ProposalComparisonView
        sessionId="test-123"
        proposals={mockProposals}
        onClose={vi.fn()}
        onSelectProposal={vi.fn()}
      />
    );

    // Select and compare
    fireEvent.click(screen.getByText('Proposal 1'));
    fireEvent.click(screen.getByText('Proposal 2'));
    fireEvent.click(screen.getByRole('button', { name: /compare selected/i }));

    await waitFor(() => {
      expect(screen.getByText(/show all side-by-side/i)).toBeInTheDocument();
    });

    // Click expand
    fireEvent.click(screen.getByText(/show all side-by-side/i));

    await waitFor(() => {
      expect(screen.getByText(/show differences only/i)).toBeInTheDocument();
    });
  });

  it('displays error message on API failure', async () => {
    vi.spyOn(api, 'compareProposals').mockRejectedValue({
      response: { status: 400, data: { detail: 'Invalid proposals' } },
    });

    render(
      <ProposalComparisonView
        sessionId="test-123"
        proposals={mockProposals}
        onClose={vi.fn()}
        onSelectProposal={vi.fn()}
      />
    );

    // Select and compare
    fireEvent.click(screen.getByText('Proposal 1'));
    fireEvent.click(screen.getByText('Proposal 2'));
    fireEvent.click(screen.getByRole('button', { name: /compare selected/i }));

    await waitFor(() => {
      expect(screen.getByText('Invalid proposals')).toBeInTheDocument();
    });
  });

  it('calls onSelectProposal and onClose when proposal selected', async () => {
    const onSelectProposal = vi.fn();
    const onClose = vi.fn();
    vi.spyOn(api, 'compareProposals').mockResolvedValue(mockComparison);

    render(
      <ProposalComparisonView
        sessionId="test-123"
        proposals={mockProposals}
        onClose={onClose}
        onSelectProposal={onSelectProposal}
      />
    );

    // Select and compare
    fireEvent.click(screen.getByText('Proposal 1'));
    fireEvent.click(screen.getByText('Proposal 2'));
    fireEvent.click(screen.getByRole('button', { name: /compare selected/i }));

    await waitFor(() => {
      expect(screen.getByText(/select this/i)).toBeInTheDocument();
    });

    // Click select on first proposal
    fireEvent.click(screen.getAllByText(/select this/i)[0]);

    expect(onSelectProposal).toHaveBeenCalledWith(1);
    expect(onClose).toHaveBeenCalled();
  });
});
EOF
```

**Step 3: Run all tests**

Backend: `uv run pytest tests/ -v --cov=src/planweaver/services/comparison_service`

Frontend: `cd frontend && npm test`

**Step 4: Fix any failing tests**

Address test failures until all pass.

**Step 5: Check test coverage**

Backend: `uv run pytest tests/ --cov=src/planweaver/services/comparison_service --cov-report=term-missing`

Target: 90%+ coverage

**Step 6: Commit**

```bash
git add tests/
git commit -m "test: add comprehensive tests for comparison feature"
```

---

## Task 9: Documentation

**Files:**
- Create: `docs/features/plan-comparison.md`
- Update: `README.md` (if needed)

**Step 1: Create feature documentation**

```bash
mkdir -p docs/features
cat > docs/features/plan-comparison.md << 'EOF'
# Plan Comparison Feature

## Overview

The Plan Comparison feature enables users to compare multiple strawman proposals side-by-side before committing to execution. This helps make informed decisions by showing execution steps, time/cost estimates, and risk assessments for each approach.

## How It Works

### 1. Lightweight Analysis (Automatic)

When strawman proposals are generated, each includes:
- **Step Count**: Estimated number of execution steps
- **Complexity Score**: Low, Medium, or High
- **Time Estimate**: Based on ~2 minutes per step
- **Cost Estimate**: Based on model pricing
- **Risk Factors**: 2-3 specific risks identified

### 2. Detailed Comparison (On-Demand)

Click "Compare Proposals" to see:
- **Diff View**: Shows differences between proposals (default)
- **Side-by-Side View**: Full execution graphs for each proposal

### 3. Proposal Selection

Select the best proposal based on:
- Total execution time
- Estimated cost
- Risk level
- Specific steps unique to each approach

## Usage

### From the UI

1. After proposals are generated, click "Compare Proposals"
2. Select 2-3 proposals to compare
3. Review the diff view showing differences
4. Optional: Expand to side-by-side for full details
5. Click "Select This" on your chosen proposal

### From the API

```python
import requests

# Compare proposals 1 and 2
response = requests.post(
    "http://localhost:8000/api/v1/sessions/{session_id}/compare-proposals",
    json={"proposal_ids": [1, 2]}
)

comparison = response.json()

# Access comparison data
print(f"Proposal 1: {comparison['time_comparison'][1]} minutes")
print(f"Proposal 2: {comparison['cost_comparison'][2]} USD")
print(f"Common steps: {len(comparison['common_steps'])}")
```

## API Reference

### POST /api/v1/sessions/{session_id}/compare-proposals

Compare detailed execution graphs for selected proposals.

**Request:**
```json
{
  "proposal_ids": [1, 2]
}
```

**Response:**
```json
{
  "session_id": "abc-123",
  "proposals": [
    {
      "proposal_id": 1,
      "full_execution_graph": [...],
      "accurate_time_estimate": 18,
      "accurate_cost_estimate": "0.08",
      "all_risk_factors": ["Data migration"]
    }
  ],
  "common_steps": [
    {"task": "Backup database", "complexity": "Low"}
  ],
  "unique_steps_by_proposal": {
    "1": [{"task": "Migrate data", "complexity": "High"}],
    "2": [{"task": "Stop production", "complexity": "Low"}]
  },
  "time_comparison": {"1": 18, "2": 12},
  "cost_comparison": {"1": "0.08", "2": "0.15"},
  "complexity_comparison": {"1": "Medium", "2": "High"}
}
```

## Implementation Details

### Caching

Execution graphs are cached for 10 minutes to avoid regenerating for repeated comparisons.

### Cost Estimation

Costs are estimated based on:
- 500 tokens per step (average)
- Model-specific pricing per 1M tokens
- Total: `(steps * 500 / 1,000,000) * price_per_m`

### Time Estimation

Times are estimated based on:
- 2 minutes per step (base)
- Complexity multiplier: Low=1.0x, Medium=1.5x, High=2.5x
- Total: `sum(step_time * complexity_multiplier)`

### Step Matching

Steps are matched between proposals using:
- Exact string matching (case-insensitive)
- Fuzzy matching for substrings (>5 chars)
- Similarity threshold: 0.85

## Performance

- **Lightweight Analysis**: <3 seconds (during proposal generation)
- **First Comparison**: <5 seconds (generates execution graphs)
- **Cached Comparison**: <500ms

## Troubleshooting

### Comparison returns "Some details unavailable"

This occurs when the LLM fails to generate execution graphs for one or more proposals. Common causes:
- Network issues
- LLM API rate limits
- Invalid proposal data

**Solution**: Wait a moment and try again. The service uses exponential backoff for retries.

### Cost estimates seem incorrect

Estimates are based on averages and may vary. Factors affecting accuracy:
- Actual step complexity differs from inference
- Model pricing changes
- Token usage varies by step

**Solution**: Treat estimates as ballpark figures. Actual costs tracked in session history.

### Steps don't match between proposals

The fuzzy matching algorithm uses substring matching. Very different wording may not match.

**Solution**: View side-by-side mode to see all steps explicitly.

## Future Enhancements

- Export comparison to PDF/Markdown
- Save comparison snapshots
- Share comparison links
- Historical accuracy tracking
EOF
```

**Step 2: Update README with feature mention**

Add to features section:

```markdown
### Plan Comparison

Compare multiple proposals side-by-side with execution steps, cost/time estimates, and risk assessments. [Learn more](docs/features/plan-comparison.md)
```

**Step 3: Commit**

```bash
git add docs/
git commit -m "docs: add Plan Comparison feature documentation"
```

---

## Task 10: Final Review and Polish

**Step 1: Run full test suite**

Backend: `uv run pytest tests/ -v`

Frontend: `cd frontend && npm test`

**Step 2: Check TypeScript compilation**

Frontend: `cd frontend && npm run type-check`

**Step 3: Manual smoke test**

1. Start backend: `uv run uvicorn planweaver.api.main:app --reload`
2. Start frontend: `cd frontend && npm run dev`
3. Create a session and generate proposals
4. Compare proposals
5. Verify diff and side-by-side views
6. Select proposal from comparison

**Step 4: Check code quality**

Backend linting: `uv run ruff check src/planweaver/services/comparison_service.py`

Frontend linting: `cd frontend && npm run lint`

**Step 5: Performance check**

- Generate comparison with 2 proposals
- Note response time (should be <5s)
- Compare same proposals again (should be <500ms from cache)

**Step 6: Final commit**

```bash
git add .
git commit -m "feat: complete Plan Comparison feature

Implement comprehensive proposal comparison with:
- Lightweight analysis (step count, complexity, time, cost, risks)
- On-demand detailed comparison with execution graphs
- Diff view (default) and side-by-side view
- Caching for performance
- Comprehensive error handling
- Full test coverage (90%+ backend, 80%+ frontend)

Complexity: Medium (7-10 days)
Impact: High (improves core planning decision point)

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

**Step 7: Push to remote**

```bash
git push origin main
```

---

## Summary

This implementation plan adds the Plan Comparison feature to PlanWeaver in 10 tasks:

1. ‚úÖ Models for comparison data structures
2. ‚úÖ Enhanced planner with lightweight analysis
3. ‚úÖ Comparison service with diff logic
4. ‚úÖ API endpoint for comparisons
5. ‚úÖ Frontend comparison components
6. ‚úÖ Integration into PlanView
7. ‚úÖ Error handling and edge cases
8. ‚úÖ Comprehensive testing
9. ‚úÖ Documentation
10. ‚úÖ Final review and polish

Each task follows TDD principles with test-first development, frequent commits, and bite-sized steps (2-5 minutes each).

**Total Estimated Time:** 7-10 days for a developer familiar with the codebase.
**Test Coverage Target:** 90%+ backend, 80%+ frontend
**Performance Targets:** <5s cold, <500ms warm cache
